# AI Agent
"""
ReACTæ¨¡å¼AI Agent - å®Œæ•´å·¥å…·é›†
"""

import json
import re
from typing import List, AsyncGenerator, Optional, Any

from sqlalchemy.ext.asyncio import AsyncSession

from app.models.settings import AIConfig
from app.schemas.ai import ChatMessage, AgentResponse, AgentThought, AgentToolCall


# å·¥å…·å®šä¹‰
# è¯´æ˜ï¼šåœ¨åŸ 12 ä¸ªå·¥å…·åŸºç¡€ä¸Šï¼Œè¡¥é½å¸‚åœºæ¦‚è§ˆ/é¾™è™æ¦œ/åŒ—å‘èµ„é‡‘ç­‰é«˜é¢‘ä¿¡æ¯ï¼Œæ–¹ä¾¿ Agent ç«¯åˆ°ç«¯å®Œæˆä¿¡æ¯æ”¶é›†ä¸å›ç­”ã€‚
TOOLS = [
    # 1. QueryStockPriceInfo - å®æ—¶è‚¡ä»·æ•°æ®
    {
        "name": "query_stock_price",
        "description": "æ‰¹é‡è·å–å®æ—¶è‚¡ä»·æ•°æ®ï¼ŒåŒ…æ‹¬å½“å‰ä»·æ ¼ã€æ¶¨è·Œå¹…ã€æˆäº¤é‡ã€æ¢æ‰‹ç‡ç­‰",
        "parameters": {
            "type": "object",
            "properties": {
                "stock_codes": {
                    "type": "string",
                    "description": "è‚¡ç¥¨ä»£ç ï¼Œå¤šä¸ªç”¨é€—å·éš”å¼€ï¼Œæ ¼å¼å¿…é¡»ä¸ºsh/sz/hkå¼€å¤´ï¼Œä¾‹å¦‚ï¼šsz399001,sh600859"
                }
            },
            "required": ["stock_codes"]
        }
    },
    # 2. QueryStockKLine - Kçº¿æ•°æ®
    {
        "name": "query_stock_kline",
        "description": "è·å–è‚¡ç¥¨Kçº¿æ•°æ®ï¼Œè¾“å…¥è‚¡ç¥¨ä»£ç å’ŒKçº¿æ¡æ•°ï¼Œè¿”å›è‚¡ç¥¨Kçº¿æ•°æ®",
        "parameters": {
            "type": "object",
            "properties": {
                "stock_code": {
                    "type": "string",
                    "description": "è‚¡ç¥¨ä»£ç ï¼Œæ ¼å¼ä¸ºAè‚¡(sh/szå¼€å¤´)ã€æ¸¯è‚¡(hkå¼€å¤´)æˆ–ç¾è‚¡(uså¼€å¤´)"
                },
                "days": {
                    "type": "integer",
                    "description": "æ—¥Kæ•°æ®æ¡æ•°",
                    "default": 30
                }
            },
            "required": ["stock_code"]
        }
    },
    # 3. QueryStockCodeInfo - è‚¡ç¥¨ä¿¡æ¯æŸ¥è¯¢
    {
        "name": "query_stock_info",
        "description": "æŸ¥è¯¢è‚¡ç¥¨/æŒ‡æ•°ä¿¡æ¯(åç§°ã€ä»£ç ã€æ‹¼éŸ³ã€æ‹¼éŸ³é¦–å­—æ¯ã€äº¤æ˜“æ‰€ç­‰)",
        "parameters": {
            "type": "object",
            "properties": {
                "search_word": {
                    "type": "string",
                    "description": "è‚¡ç¥¨æœç´¢å…³é”®è¯"
                }
            },
            "required": ["search_word"]
        }
    },
    # 4. GetFinancialReport - è´¢åŠ¡æŠ¥è¡¨
    {
        "name": "get_financial_report",
        "description": "æŸ¥è¯¢è‚¡ç¥¨è´¢åŠ¡æŠ¥è¡¨æ•°æ®ï¼ŒåŒ…æ‹¬åˆ©æ¶¦è¡¨ã€èµ„äº§è´Ÿå€ºè¡¨ç­‰",
        "parameters": {
            "type": "object",
            "properties": {
                "stock_code": {
                    "type": "string",
                    "description": "è‚¡ç¥¨ä»£ç ï¼Œæ ¼å¼ä¸ºAè‚¡(sh/szå¼€å¤´)ã€æ¸¯è‚¡(hkå¼€å¤´)æˆ–ç¾è‚¡(uså¼€å¤´)ï¼Œä¸æ”¯æŒæ‰¹é‡æŸ¥è¯¢"
                }
            },
            "required": ["stock_code"]
        }
    },
    # 5. ChoiceStockByIndicators - è‡ªç„¶è¯­è¨€é€‰è‚¡
    {
        "name": "choice_stock_by_indicators",
        "description": "æ ¹æ®è‡ªç„¶è¯­è¨€ç­›é€‰è‚¡ç¥¨ï¼Œè¿”å›è‡ªç„¶è¯­è¨€é€‰è‚¡æ¡ä»¶è¦æ±‚çš„è‚¡ç¥¨æ‰€æœ‰ç›¸å…³æ•°æ®ã€‚æ”¯æŒæŠ€æœ¯æŒ‡æ ‡(MACDã€RSIã€KDJã€BOLL)ã€å‡çº¿ã€å¸‚å€¼ã€æ¢æ‰‹ç‡ã€æ¶¨å¹…ç­‰æ¡ä»¶",
        "parameters": {
            "type": "object",
            "properties": {
                "words": {
                    "type": "string",
                    "description": "é€‰è‚¡è‡ªç„¶è¯­è¨€ï¼Œå¦‚ï¼šæ¶¨åœè‚¡ã€ä¸»åŠ›èµ„é‡‘æµå…¥ã€MACDé‡‘å‰ã€å¸‚ç›ˆç‡ä½äº20ç­‰"
                }
            },
            "required": ["words"]
        }
    },
    # 6. QueryMarketNews - å¸‚åœºèµ„è®¯
    {
        "name": "query_market_news",
        "description": "å›½å†…å¤–å¸‚åœºèµ„è®¯/ç”µæŠ¥/ä¼šè®®/äº‹ä»¶ï¼Œè¿”å›äº‹ä»¶æ—¥æœŸã€å¸‚åœºèµ„è®¯ã€å…¨çƒæ–°é—»ã€å¤–åª’æ–°é—»ç­‰",
        "parameters": {
            "type": "object",
            "properties": {
                "limit": {
                    "type": "integer",
                    "description": "è¿”å›æ•°é‡",
                    "default": 20
                }
            }
        }
    },
    # 7. QueryStockNewsTool - è‚¡ç¥¨æ–°é—»æœç´¢
    {
        "name": "query_stock_news",
        "description": "æŒ‰å…³é”®è¯æœç´¢ç›¸å…³å¸‚åœºèµ„è®¯/æ–°é—»",
        "parameters": {
            "type": "object",
            "properties": {
                "search_words": {
                    "type": "string",
                    "description": "æœç´¢å…³é”®è¯ï¼Œå¤šä¸ªå…³é”®è¯ä½¿ç”¨ç©ºæ ¼åˆ†éš”"
                }
            },
            "required": ["search_words"]
        }
    },
    # 8. QueryInteractiveAnswerData - æŠ•èµ„è€…äº’åŠ¨é—®ç­”
    {
        "name": "query_interactive_qa",
        "description": "è·å–æŠ•èµ„è€…ä¸ä¸Šå¸‚å…¬å¸äº’åŠ¨é—®ç­”çš„æ•°æ®ï¼Œåæ˜ å½“å‰æŠ•èµ„è€…å…³æ³¨çš„çƒ­ç‚¹é—®é¢˜",
        "parameters": {
            "type": "object",
            "properties": {
                "page": {
                    "type": "integer",
                    "description": "åˆ†é¡µå·",
                    "default": 1
                },
                "page_size": {
                    "type": "integer",
                    "description": "åˆ†é¡µå¤§å°",
                    "default": 20
                },
                "keyword": {
                    "type": "string",
                    "description": "æœç´¢å…³é”®è¯ï¼Œå¤šä¸ªå…³é”®è¯ç©ºæ ¼éš”å¼€ï¼ˆå¯è¾“å…¥è‚¡ç¥¨åç§°æˆ–çƒ­é—¨æ¿å—/è¡Œä¸š/æ¦‚å¿µç­‰ï¼‰"
                }
            }
        }
    },
    # 9. GetIndustryResearchReport - è¡Œä¸šç ”ç©¶æŠ¥å‘Š
    {
        "name": "get_industry_research_report",
        "description": "è·å–è¡Œä¸š/æ¿å—ç ”ç©¶æŠ¥å‘Š",
        "parameters": {
            "type": "object",
            "properties": {
                "name": {
                    "type": "string",
                    "description": "è¡Œä¸š/æ¿å—åç§°"
                },
                "code": {
                    "type": "string",
                    "description": "è¡Œä¸š/æ¿å—ä»£ç "
                }
            }
        }
    },
    # 10. QueryEconomicData - å®è§‚ç»æµæ•°æ®
    {
        "name": "query_economic_data",
        "description": "æŸ¥è¯¢å®è§‚ç»æµæ•°æ®(GDPã€CPIã€PPIã€PMI)",
        "parameters": {
            "type": "object",
            "properties": {
                "flag": {
                    "type": "string",
                    "description": "æ•°æ®ç±»å‹: all(å…¨éƒ¨), GDP(å›½å†…ç”Ÿäº§æ€»å€¼), CPI(å±…æ°‘æ¶ˆè´¹ä»·æ ¼æŒ‡æ•°), PPI(å·¥ä¸šå“å‡ºå‚ä»·æ ¼æŒ‡æ•°), PMI(é‡‡è´­ç»ç†äººæŒ‡æ•°)",
                    "default": "all"
                }
            }
        }
    },
    # 11. QueryBKDictInfo - æ¿å—/è¡Œä¸šå­—å…¸
    {
        "name": "query_bk_dict",
        "description": "è·å–æ‰€æœ‰æ¿å—/è¡Œä¸šåç§°æˆ–è€…ä»£ç (bkCode,bkName)",
        "parameters": {
            "type": "object",
            "properties": {}
        }
    },
    # 12. èµ„é‡‘æµå‘æ’å
    {
        "name": "get_money_flow_rank",
        "description": "è·å–èµ„é‡‘æµå‘æ’åï¼Œä¸»åŠ›èµ„é‡‘å‡€æµå…¥/æµå‡ºæ’è¡Œ",
        "parameters": {
            "type": "object",
            "properties": {
                "limit": {
                    "type": "integer",
                    "description": "è¿”å›æ•°é‡",
                    "default": 20
                },
                "order": {
                    "type": "string",
                    "description": "æ’åºæ–¹å‘: desc(æµå…¥), asc(æµå‡º)",
                    "default": "desc"
                }
            }
        }
    },
    # 13. query_market_overview - å¸‚åœºæ¦‚è§ˆï¼ˆå¯¹é½ daily_stock_analysis çš„å¤ç›˜å£å¾„ï¼‰
    {
        "name": "query_market_overview",
        "description": "è·å–å¸‚åœºæ¦‚è§ˆï¼šæŒ‡æ•°ã€æ¶¨è·Œå®¶æ•°ã€æˆäº¤é¢ã€æ¶¨è·Œåœã€æ¿å—æ¶¨è·Œæ¦œã€ï¼ˆå¯é€‰ï¼‰åŒ—å‘èµ„é‡‘ç­‰",
        "parameters": {"type": "object", "properties": {}}
    },
    # 14. query_long_tiger - é¾™è™æ¦œ
    {
        "name": "query_long_tiger",
        "description": "è·å–é¾™è™æ¦œï¼ˆå¯æŒ‡å®šäº¤æ˜“æ—¥æœŸ YYYY-MM-DDï¼‰",
        "parameters": {
            "type": "object",
            "properties": {
                "trade_date": {"type": "string", "description": "äº¤æ˜“æ—¥æœŸ YYYY-MM-DDï¼ˆå¯é€‰ï¼‰"}
            }
        }
    },
    # 15. query_north_flow - åŒ—å‘èµ„é‡‘
    {
        "name": "query_north_flow",
        "description": "è·å–åŒ—å‘èµ„é‡‘ï¼ˆæ²ªè‚¡é€š/æ·±è‚¡é€šï¼‰å†å²æ•°æ®",
        "parameters": {
            "type": "object",
            "properties": {
                "days": {"type": "integer", "description": "è¿‘ N ä¸ªäº¤æ˜“æ—¥", "default": 30}
            }
        }
    },
    # 16. query_industry_rank - è¡Œä¸šæ¶¨è·Œæ¦œ
    {
        "name": "query_industry_rank",
        "description": "è·å–è¡Œä¸šæ’åï¼ˆæ¶¨å¹…/æ¢æ‰‹ç‡ç­‰ï¼‰",
        "parameters": {
            "type": "object",
            "properties": {
                "sort_by": {"type": "string", "description": "change_percent/turnover", "default": "change_percent"},
                "order": {"type": "string", "description": "asc/desc", "default": "desc"},
                "limit": {"type": "integer", "description": "è¿”å›æ•°é‡", "default": 20}
            }
        }
    },
    # 17. query_concept_rank - æ¦‚å¿µæ¿å—æ¶¨è·Œæ¦œ
    {
        "name": "query_concept_rank",
        "description": "è·å–æ¦‚å¿µæ¿å—æ’åï¼ˆæ¶¨å¹…/æ¢æ‰‹ç‡ç­‰ï¼‰",
        "parameters": {
            "type": "object",
            "properties": {
                "sort_by": {"type": "string", "description": "change_percent/turnover", "default": "change_percent"},
                "order": {"type": "string", "description": "asc/desc", "default": "desc"},
                "limit": {"type": "integer", "description": "è¿”å›æ•°é‡", "default": 20}
            }
        }
    },
    # 18. query_industry_money_flow - è¡Œä¸š/æ¦‚å¿µèµ„é‡‘æµå‘æ¦œ
    {
        "name": "query_industry_money_flow",
        "description": "è·å–è¡Œä¸š/æ¦‚å¿µèµ„é‡‘æµå‘æ’å",
        "parameters": {
            "type": "object",
            "properties": {
                "category": {"type": "string", "description": "hangye(è¡Œä¸š)/gainian(æ¦‚å¿µ)", "default": "hangye"},
                "sort_by": {"type": "string", "description": "æ’åºå­—æ®µ", "default": "main_inflow"}
            }
        }
    },
    # 19. query_stock_money_rank - ä¸ªè‚¡èµ„é‡‘æµå…¥æ¦œ
    {
        "name": "query_stock_money_rank",
        "description": "è·å–è‚¡ç¥¨èµ„é‡‘æµå…¥æ’å",
        "parameters": {
            "type": "object",
            "properties": {
                "sort_by": {"type": "string", "description": "æ’åºå­—æ®µ", "default": "main_inflow"},
                "limit": {"type": "integer", "description": "è¿”å›æ•°é‡", "default": 20}
            }
        }
    },
    # 20. query_volume_ratio_rank - é‡æ¯”æ’å
    {
        "name": "query_volume_ratio_rank",
        "description": "è·å–é‡æ¯”æ’åï¼ˆç”¨äºå‘ç°é‡èƒ½å¼‚åŠ¨ï¼‰",
        "parameters": {
            "type": "object",
            "properties": {
                "min_ratio": {"type": "number", "description": "æœ€å°é‡æ¯”", "default": 2.0},
                "limit": {"type": "integer", "description": "è¿”å›æ•°é‡", "default": 20}
            }
        }
    },
    # 21. query_limit_stats - æ¶¨è·Œåœç»Ÿè®¡
    {
        "name": "query_limit_stats",
        "description": "è·å–æ¶¨åœ/è·Œåœç»Ÿè®¡ä¸åå•",
        "parameters": {"type": "object", "properties": {}}
    },
    # 22. get_stock_detail - è‚¡ç¥¨å…¨é‡è¯¦æƒ…ï¼ˆä¼°å€¼/è´¢åŠ¡/è‚¡ä¸œ/åˆ†çº¢ç­‰ï¼‰
    {
        "name": "get_stock_detail",
        "description": "è·å–è‚¡ç¥¨å®Œæ•´è¯¦æƒ…ï¼ˆä¼°å€¼æŒ‡æ ‡ã€è´¢åŠ¡ã€æœºæ„è¯„çº§ã€è‚¡ä¸œã€åˆ†çº¢ã€èµ„é‡‘æµå‘ç­‰ï¼›å®é™…å­—æ®µä»¥æ•°æ®æºå¯ç”¨æ€§ä¸ºå‡†ï¼‰",
        "parameters": {
            "type": "object",
            "properties": {
                "stock_code": {"type": "string", "description": "è‚¡ç¥¨ä»£ç ï¼Œæ ¼å¼ä¸ºAè‚¡(sh/szå¼€å¤´)ã€æ¸¯è‚¡(hkå¼€å¤´)æˆ–ç¾è‚¡(uså¼€å¤´)"}
            },
            "required": ["stock_code"]
        }
    },
    # 23. query_chip_distribution - ç­¹ç åˆ†å¸ƒï¼ˆå¯¹é½ daily_stock_analysisï¼‰
    {
        "name": "query_chip_distribution",
        "description": "è·å–ç­¹ç åˆ†å¸ƒï¼ˆè·åˆ©æ¯”ä¾‹ã€å¹³å‡æˆæœ¬ã€70/90 æˆæœ¬åŒºé—´ä¸é›†ä¸­åº¦ï¼‰ã€‚æ³¨ï¼šETF/æŒ‡æ•°/éƒ¨åˆ†è‚¡ç¥¨å¯èƒ½æ— æ•°æ®",
        "parameters": {
            "type": "object",
            "properties": {
                "stock_code": {"type": "string", "description": "è‚¡ç¥¨ä»£ç ï¼ŒAè‚¡å»ºè®® sh/sz å‰ç¼€ï¼›ä¹Ÿæ”¯æŒ 6 ä½æ•°å­—è‡ªåŠ¨è¯†åˆ«"}
            },
            "required": ["stock_code"]
        }
    },
    # 24. query_technical_analysis - æŠ€æœ¯æŒ‡æ ‡ä¸ä¿¡å·ï¼ˆMA/MACD/RSI/æ”¯æ’‘å‹åŠ›/è¯„åˆ†ï¼‰
    {
        "name": "query_technical_analysis",
        "description": "è®¡ç®—æŠ€æœ¯æŒ‡æ ‡ï¼ˆMA/MACD/RSI/é‡æ¯”/æ”¯æ’‘å‹åŠ›ï¼‰å¹¶ç»™å‡ºç»¼åˆè¯„åˆ†ä¸ä¹°å–ä¿¡å·ï¼ˆç”¨äºç”Ÿæˆå†³ç­–ä»ªè¡¨ç›˜ï¼‰",
        "parameters": {
            "type": "object",
            "properties": {
                "stock_code": {"type": "string", "description": "è‚¡ç¥¨ä»£ç ï¼ŒAè‚¡å»ºè®® sh/sz å‰ç¼€ï¼›ä¹Ÿæ”¯æŒ 6 ä½æ•°å­—è‡ªåŠ¨è¯†åˆ«"},
                "days": {"type": "integer", "description": "ç”¨äºè®¡ç®—çš„æ—¥Kæ¡æ•°ï¼ˆéœ€>=60ï¼‰", "default": 120}
            },
            "required": ["stock_code"]
        }
    },
    # 25. query_stock_notices - å…¬å¸å…¬å‘Š
    {
        "name": "query_stock_notices",
        "description": "è·å–å…¬å¸å…¬å‘Šï¼ˆç”¨äºé£é™©æ’æŸ¥ï¼šå‡æŒ/å›è´­/ç«‹æ¡ˆ/å¤„ç½š/ä¸šç»©é¢„å‘Šç­‰ï¼‰",
        "parameters": {
            "type": "object",
            "properties": {
                "stock_code": {"type": "string", "description": "è‚¡ç¥¨ä»£ç "},
                "limit": {"type": "integer", "description": "è¿”å›æ•°é‡", "default": 20}
            },
            "required": ["stock_code"]
        }
    },
    # 26. query_stock_research_reports - ç ”æŠ¥/æœºæ„æŠ¥å‘Š
    {
        "name": "query_stock_research_reports",
        "description": "è·å–è‚¡ç¥¨ç ”æŠ¥ï¼ˆæ ‡é¢˜/æœºæ„/æ—¶é—´/æ‘˜è¦ç­‰ï¼‰ï¼Œç”¨äºè¡¥å……æœºæ„è§‚ç‚¹ä¸é¢„æœŸå·®",
        "parameters": {
            "type": "object",
            "properties": {
                "stock_code": {"type": "string", "description": "è‚¡ç¥¨ä»£ç "},
                "limit": {"type": "integer", "description": "è¿”å›æ•°é‡", "default": 10}
            },
            "required": ["stock_code"]
        }
    },
    # 27. query_hot_topics - çƒ­é—¨è¯é¢˜
    {
        "name": "query_hot_topics",
        "description": "è·å–å¸‚åœºçƒ­é—¨è¯é¢˜ï¼ˆç”¨äºåˆ¤æ–­çŸ­çº¿æƒ…ç»ªä¸ä¸»çº¿ï¼‰",
        "parameters": {"type": "object", "properties": {"size": {"type": "integer", "default": 20}}}
    },
    # 28. query_hot_events - çƒ­é—¨äº‹ä»¶
    {
        "name": "query_hot_events",
        "description": "è·å–å¸‚åœºçƒ­é—¨äº‹ä»¶ï¼ˆç”¨äºæ•æ‰è¿‘æœŸå‚¬åŒ–ï¼‰",
        "parameters": {"type": "object", "properties": {"size": {"type": "integer", "default": 20}}}
    },
    # 29. search_web - è”ç½‘æœç´¢ï¼ˆå¤šå¼•æ“ï¼‰
    {
        "name": "search_web",
        "description": "è”ç½‘æ£€ç´¢èµ„è®¯ï¼ˆå¤šå¼•æ“/å¤šKey/è‡ªåŠ¨å»é‡æ’åºï¼‰ï¼Œè¿”å›ç»“æ„åŒ–ç»“æœä¸å¯è¯»ä¸Šä¸‹æ–‡",
        "parameters": {
            "type": "object",
            "properties": {
                "query": {"type": "string", "description": "æœç´¢å…³é”®è¯/é—®é¢˜"},
                "limit": {"type": "integer", "description": "è¿”å›æ•°é‡", "default": 8}
            },
            "required": ["query"]
        }
    },
]


class StockAgent:
    """è‚¡ç¥¨åˆ†æAgent"""

    def __init__(self, config: AIConfig, db: AsyncSession):
        self.config = config
        self.db = db

    @staticmethod
    def _tool_name_set() -> set[str]:
        """å¯ç”¨å·¥å…·åé›†åˆï¼ˆç”¨äºè§„åˆ’/allow çº¦æŸè¿‡æ»¤ï¼‰ã€‚"""
        return {str(t.get("name", "") or "").strip() for t in TOOLS if t.get("name")}

    def _build_planner_prompt(
        self,
        *,
        mode: str,
        max_steps: int,
        candidate_hint: str = "",
        retrieval_context: str = "",
    ) -> str:
        """æ„å»º Plan é˜¶æ®µ Promptï¼ˆå¯¹é½ LearningSelfAgent çš„ Plan-ReAct èŒƒå¼ï¼‰ã€‚"""
        tools_desc = json.dumps(
            [
                {
                    "name": t.get("name"),
                    "description": t.get("description"),
                    "parameters": t.get("parameters"),
                }
                for t in TOOLS
            ],
            ensure_ascii=False,
            indent=2,
        )

        hint = (candidate_hint or "").strip()
        hint_block = f"\nè§„åˆ’é£æ ¼æç¤ºï¼š{hint}\n" if hint else ""

        rc = (retrieval_context or "").strip()
        retrieval_block = f"\nå·²æ£€ç´¢åˆ°çš„çŸ¥è¯†ï¼ˆä¾›è§„åˆ’å‚è€ƒï¼Œå¯èƒ½ä¸å®Œæ•´ï¼‰ï¼š\n{rc}\n" if rc else ""

        return f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä»»åŠ¡è§„åˆ’å™¨ï¼ˆPlannerï¼‰ã€‚ä½ çš„èŒè´£æ˜¯ï¼šæ ¹æ®ç”¨æˆ·é—®é¢˜ï¼Œç”Ÿæˆä¸€ä¸ªå¯æ‰§è¡Œçš„è®¡åˆ’ï¼ˆPlanï¼‰ï¼Œä¾›åç»­ ReAct æ‰§è¡Œå™¨é€æ­¥å®Œæˆã€‚

ä½ å¯ç”¨çš„å·¥å…·å¦‚ä¸‹ï¼ˆä»…å…è®¸ä» tool.name é€‰æ‹©ï¼‰ï¼š
{tools_desc}
{retrieval_block}
{hint_block}
è¾“å‡ºè¦æ±‚ï¼ˆå¿…é¡»ä¸¥æ ¼éµå®ˆï¼‰ï¼š
- åªè¾“å‡ºä¸€ä¸ª JSON å¯¹è±¡ï¼Œä¸è¦è¾“å‡ºä»»ä½•é¢å¤–æ–‡å­—ã€è§£é‡Šæˆ– Markdownã€‚
- JSON schemaï¼š
{{
  "mode": "{mode}",
  "steps": [
    {{
      "title": "æ­¥éª¤æ ‡é¢˜ï¼ˆçŸ­ï¼‰",
      "goal": "æœ¬æ­¥éª¤ç›®æ ‡ï¼ˆæ˜ç¡®ã€å¯éªŒè¯ï¼‰",
      "allowed_tools": ["tool_name1", "tool_name2"]
    }}
  ]
}}

è§„åˆ’è§„åˆ™ï¼š
- steps æ•°é‡ <= {max_steps}ï¼ˆè¶Šå°‘è¶Šå¥½ï¼Œä½†è¦è¦†ç›–å®Œæˆä»»åŠ¡æ‰€éœ€ä¿¡æ¯ï¼‰ã€‚
- allowed_tools åªèƒ½æ˜¯ä¸Šé¢å·¥å…·åˆ—è¡¨ä¸­çš„ nameï¼›æ¯æ­¥å»ºè®® 0-4 ä¸ªå·¥å…·ã€‚
- è‹¥ç”¨æˆ·é—®é¢˜ä¸éœ€è¦è°ƒç”¨ä»»ä½•å·¥å…·å³å¯å›ç­”ï¼ˆé—²èŠ/çº¯çŸ¥è¯†è§£é‡Š/ä¸ä¾èµ–å®æ—¶æ•°æ®ï¼‰ï¼Œsteps å¯ä»¥ä¸ºç©ºåˆ—è¡¨ã€‚
- è®¡åˆ’è¦å°½é‡ä½“ç°ï¼šä¿¡æ¯æ”¶é›† â†’ å…³é”®åˆ¤æ–­ â†’ æ±‡æ€»è¾“å‡º çš„é¡ºåºã€‚
"""

    def _parse_plan(self, content: str, *, max_steps: int) -> Optional[dict]:
        """è§£æ Planner è¾“å‡ºçš„è®¡åˆ’ JSONï¼ˆå®¹é”™ï¼šå…è®¸åŒ…è£¹æ–‡æœ¬/ä»£ç å—ï¼‰ã€‚"""
        text = (content or "").strip()
        if not text:
            return None

        # 1) ä»£ç å—ä¼˜å…ˆ
        fence = re.search(r"```(?:json)?\s*(.*?)\s*```", text, flags=re.IGNORECASE | re.DOTALL)
        if fence:
            text = (fence.group(1) or "").strip()

        # 2) ç›´æ¥è§£æ
        raw_obj: Optional[dict] = None
        try:
            parsed = json.loads(text)
            raw_obj = parsed if isinstance(parsed, dict) else None
        except Exception:
            raw = self._extract_json_object(text)
            if raw:
                try:
                    parsed = json.loads(raw)
                    raw_obj = parsed if isinstance(parsed, dict) else None
                except Exception:
                    raw_obj = None

        if not raw_obj:
            return None

        steps = raw_obj.get("steps")
        if not isinstance(steps, list):
            steps = []

        tool_names = self._tool_name_set()
        cleaned_steps: list[dict] = []
        for s in steps:
            if not isinstance(s, dict):
                continue
            title = str(s.get("title", "") or "").strip() or "æ­¥éª¤"
            goal = str(s.get("goal", "") or "").strip()
            allowed = s.get("allowed_tools", [])
            if not isinstance(allowed, list):
                allowed = []
            allowed_clean = []
            for a in allowed:
                name = self._clean_tool_name(str(a or ""))
                if not name:
                    continue
                if name in tool_names:
                    allowed_clean.append(name)
            # å»é‡ä½†ä¿æŒé¡ºåº
            dedup: list[str] = []
            seen: set[str] = set()
            for n in allowed_clean:
                if n in seen:
                    continue
                seen.add(n)
                dedup.append(n)
            cleaned_steps.append(
                {
                    "title": title[:80],
                    "goal": goal[:300],
                    "allowed_tools": dedup[:8],
                }
            )

        # æ§åˆ¶æ­¥æ•°ä¸Šé™
        max_n = max(1, int(max_steps or 1))
        cleaned_steps = cleaned_steps[:max_n]

        return {"mode": str(raw_obj.get("mode", "") or "").strip(), "steps": cleaned_steps}

    async def _create_plan(
        self,
        messages: List[ChatMessage],
        *,
        mode: str,
        max_steps: int,
        candidate_hint: str = "",
        retrieval_context: str = "",
    ) -> Optional[dict]:
        """ç”Ÿæˆè®¡åˆ’ï¼ˆå¤±è´¥æ—¶è¿”å› Noneï¼Œè°ƒç”¨æ–¹åº”è‡ªåŠ¨é™çº§ï¼‰ã€‚"""
        from app.llm.client import LLMClient

        # è§„åˆ’åªéœ€è¦æœ€è¿‘ä¸Šä¸‹æ–‡ï¼Œé¿å…æŠŠå¤§é‡ Observation æ³¨å…¥å¯¼è‡´æˆæœ¬è†¨èƒ€
        ctx = messages[-8:] if messages else []
        planner_messages = [
            ChatMessage(
                role="system",
                content=self._build_planner_prompt(
                    mode=mode,
                    max_steps=max_steps,
                    candidate_hint=candidate_hint,
                    retrieval_context=retrieval_context,
                ),
            )
        ] + ctx

        client = LLMClient(self.config)
        try:
            resp = await client.chat(planner_messages)
            return self._parse_plan(resp.response, max_steps=max_steps)
        finally:
            await client.close()

    async def _select_best_plan(self, messages: List[ChatMessage], plans: List[dict]) -> Optional[dict]:
        """è¯„ä¼°æŒ‘é€‰æœ€ä¼˜è®¡åˆ’ï¼ˆthink æ¨¡å¼çš„ç®€åŒ– evaluatorï¼‰ã€‚"""
        from app.llm.client import LLMClient

        if not plans:
            return None
        if len(plans) == 1:
            return plans[0]

        prompt = f"""ä½ æ˜¯è¯„ä¼° Agentï¼ˆEvaluatorï¼‰ã€‚è¯·ä»å¤šä¸ªå€™é€‰è®¡åˆ’ä¸­é€‰æ‹©ä¸€ä¸ªæœ€ä¼˜è®¡åˆ’ï¼Œå¹¶å¯åšè½»é‡ä¿®æ­£ä»¥æé«˜å¯æ‰§è¡Œæ€§ã€‚

è¯„ä¼°æ ‡å‡†ï¼š
- è¦†ç›–å®Œæˆä»»åŠ¡æ‰€éœ€ä¿¡æ¯ï¼ˆä¸è¿‡åº¦å†—ä½™ï¼‰
- æ­¥éª¤é¡ºåºåˆç†ï¼ˆå…ˆæ”¶é›†ä¿¡æ¯å†è¾“å‡ºï¼‰
- æ¯æ­¥ allowed_tools åˆç†ä¸”æ•°é‡å°‘ï¼ˆ0-4 ä¸ªä¸ºä½³ï¼‰
- ä¸è°ƒç”¨ä¸å­˜åœ¨çš„å·¥å…·

è¾“å‡ºè¦æ±‚ï¼š
- åªè¾“å‡ºä¸€ä¸ª JSON å¯¹è±¡ï¼Œä¸è¦è¾“å‡ºä»»ä½•é¢å¤–æ–‡å­—ã€‚
- schemaï¼š
{{
  "selected_index": 0,
  "plan": {{ "mode": "think", "steps": [...] }}
}}
"""

        eval_messages = [
            ChatMessage(role="system", content=prompt),
            ChatMessage(role="user", content=json.dumps({"candidates": plans}, ensure_ascii=False)),
        ]

        client = LLMClient(self.config)
        try:
            resp = await client.chat(eval_messages)
            obj = self._parse_action_input(resp.response)  # å¤ç”¨ JSON å®¹é”™è§£æ
            idx = obj.get("selected_index")
            plan = obj.get("plan")
            try:
                idx_int = int(idx)
            except Exception:
                idx_int = 0
            if isinstance(plan, dict) and isinstance(plan.get("steps"), list):
                parsed = self._parse_plan(json.dumps(plan, ensure_ascii=False), max_steps=len(plan.get("steps") or []))
                return parsed or plans[min(max(idx_int, 0), len(plans) - 1)]
            return plans[min(max(idx_int, 0), len(plans) - 1)]
        finally:
            await client.close()

    @staticmethod
    def _parse_step_done(content: str) -> str:
        """è§£ææ­¥éª¤å®Œæˆæ ‡è®°ï¼ˆç”¨äº Plan-ReActï¼‰ã€‚"""
        if not content:
            return ""
        m = re.search(
            r"^(?:Step\s*Done|StepDone|æ­¥éª¤å®Œæˆ|æ­¥éª¤\s*Done)\s*[:ï¼š]\s*(.+)$",
            content,
            flags=re.IGNORECASE | re.MULTILINE,
        )
        return (m.group(1) or "").strip() if m else ""

    async def run_do(
        self,
        messages: List[ChatMessage],
        *,
        max_plan_steps: int = 6,
        knowledge_context: str = "",
    ) -> AgentResponse:
        """do æ¨¡å¼ï¼šPlanï¼ˆå•æ¨¡å‹è§„åˆ’ï¼‰ + ReActï¼ˆå•æ¨¡å‹æ‰§è¡Œï¼‰ã€‚

        è¯´æ˜ï¼šè‹¥è§„åˆ’å¤±è´¥åˆ™è‡ªåŠ¨é™çº§ä¸ºç›´æ¥ ReActï¼ˆrunï¼‰ã€‚
        """
        plan = await self._create_plan(
            messages,
            mode="do",
            max_steps=max_plan_steps,
            retrieval_context=knowledge_context,
        )
        if not plan or not isinstance(plan.get("steps"), list):
            return await self.run(messages)
        return await self._run_with_plan(messages, plan, mode="do", knowledge_context=knowledge_context)

    async def run_think(
        self,
        messages: List[ChatMessage],
        *,
        max_plan_steps: int = 6,
        plan_candidates: int = 3,
        knowledge_context: str = "",
    ) -> AgentResponse:
        """think æ¨¡å¼ï¼ˆç®€åŒ–ç‰ˆï¼‰ï¼šå¤šå€™é€‰è§„åˆ’ + evaluator æŒ‘é€‰ + ReAct æ‰§è¡Œã€‚

        è¯´æ˜ï¼š
        - å½“å‰å®ç°ä¸ºâ€œåŒä¸€æ¨¡å‹å¤šè§†è§’è§„åˆ’â€ï¼Œç”¨äºå¯¹é½èŒƒå¼ï¼›ä¸å¼•å…¥é¢å¤–æ¨¡å‹ä¾èµ–ã€‚
        - è‹¥è§„åˆ’/è¯„ä¼°å¤±è´¥åˆ™è‡ªåŠ¨é™çº§ä¸º do/runã€‚
        """
        n = max(1, min(5, int(plan_candidates or 1)))
        hints = [
            "åå‘æ•°æ®å……åˆ†æ€§ï¼šå¤šç»´åº¦æ”¶é›†åå†ç»“è®º",
            "åå‘æˆæœ¬æ§åˆ¶ï¼šæœ€å°‘å·¥å…·è°ƒç”¨å®Œæˆä»»åŠ¡",
            "åå‘é£é™©æ’æŸ¥ï¼šä¼˜å…ˆå…¬å‘Š/èµ„é‡‘/å¼‚åŠ¨",
            "åå‘æŠ€æœ¯åˆ†æï¼šKçº¿/æŒ‡æ ‡/æ”¯æ’‘å‹åŠ›æ›´æ·±å…¥",
            "åå‘åŸºæœ¬é¢ï¼šè´¢åŠ¡/ä¼°å€¼/æœºæ„ç ”æŠ¥æ›´æ·±å…¥",
        ]
        plans: list[dict] = []
        for i in range(n):
            p = await self._create_plan(
                messages,
                mode="think",
                max_steps=max_plan_steps,
                candidate_hint=hints[i % len(hints)],
                retrieval_context=knowledge_context,
            )
            if p and isinstance(p.get("steps"), list) and p.get("steps"):
                plans.append(p)

        if not plans:
            # è§„åˆ’å¤±è´¥ï¼šé™çº§ä¸º doï¼ˆä»å¯èƒ½ç”Ÿæˆ planï¼‰ï¼Œå†ä¸è¡Œå°± run
            return await self.run_do(messages, max_plan_steps=max_plan_steps)

        best = await self._select_best_plan(messages, plans) or plans[0]
        return await self._run_with_plan(messages, best, mode="think", knowledge_context=knowledge_context)

    async def _run_with_plan(
        self,
        messages: List[ChatMessage],
        plan: dict,
        *,
        mode: str,
        knowledge_context: str = "",
    ) -> AgentResponse:
        """æŒ‰è®¡åˆ’æ‰§è¡Œï¼ˆæ¯æ­¥å¸¦ allow çº¦æŸï¼‰ã€‚"""
        from app.llm.client import LLMClient

        client = LLMClient(self.config)
        thoughts: list[AgentThought] = []
        tool_calls: list[AgentToolCall] = []

        steps = plan.get("steps") if isinstance(plan, dict) else None
        if not isinstance(steps, list) or not steps:
            return await self.run(messages)

        # è®°å½• planï¼ˆä¾¿äºè°ƒè¯•/å®¡è®¡ï¼‰
        try:
            thoughts.append(AgentThought(thought=f"Plan({mode})", observation=json.dumps(plan, ensure_ascii=False)))
        except Exception:
            thoughts.append(AgentThought(thought=f"Plan({mode})", observation="{}"))

        all_messages = [ChatMessage(role="system", content=self._build_system_prompt())]
        kc = (knowledge_context or "").strip()
        if kc:
            all_messages.append(ChatMessage(role="system", content=f"ã€çŸ¥è¯†æ£€ç´¢ã€‘\n{kc}"))
        all_messages += list(messages or [])

        tool_names = self._tool_name_set()

        # æ¯æ­¥æœ€å¤šè¿­ä»£æ¬¡æ•°ï¼šé¿å…é•¿å¯¹è¯åœ¨ do/think ä¸‹æˆæœ¬å¤±æ§
        step_max_iterations = 4
        final_max_iterations = 4

        # ç»Ÿè®¡ union allowï¼šç”¨äºæœ€ç»ˆæ±‡æ€»é˜¶æ®µï¼ˆé¿å…æœ€åä¸€æ­¥æƒ³è¡¥ä¸€ä¸ªæ•°æ®å´è¢«å¡æ­»ï¼‰
        union_allowed: set[str] = set()
        for s in steps:
            if not isinstance(s, dict):
                continue
            allowed = s.get("allowed_tools", [])
            if not isinstance(allowed, list):
                continue
            for a in allowed:
                name = self._clean_tool_name(str(a or ""))
                if name in tool_names:
                    union_allowed.add(name)

        try:
            # æŒ‰æ­¥éª¤æ‰§è¡Œ
            for idx, s in enumerate(steps):
                if not isinstance(s, dict):
                    continue
                title = str(s.get("title", "") or f"æ­¥éª¤{idx+1}").strip()
                goal = str(s.get("goal", "") or "").strip()
                allowed = s.get("allowed_tools", [])
                if not isinstance(allowed, list):
                    allowed = []
                allowed_step = []
                for a in allowed:
                    name = self._clean_tool_name(str(a or ""))
                    if name in tool_names:
                        allowed_step.append(name)
                # å»é‡
                allowed_step = list(dict.fromkeys(allowed_step))

                all_messages.append(
                    ChatMessage(
                        role="user",
                        content=(
                            f"ç°åœ¨æ‰§è¡Œè®¡åˆ’æ­¥éª¤ {idx+1}/{len(steps)}ã€‚\n"
                            f"- æ ‡é¢˜: {title}\n"
                            f"- ç›®æ ‡: {goal}\n"
                            f"- å…è®¸å·¥å…·: {', '.join(allowed_step) if allowed_step else 'ï¼ˆä¸å…è®¸è°ƒç”¨å·¥å…·ï¼‰'}\n\n"
                            "è¦æ±‚ï¼š\n"
                            "1) è‹¥éœ€è¦è°ƒç”¨å·¥å…·ï¼ŒAction å¿…é¡»åœ¨å…è®¸å·¥å…·åˆ—è¡¨ä¸­ï¼›å¦åˆ™è¯·ç›´æ¥æ€è€ƒå¹¶è¾“å‡º Step Doneã€‚\n"
                            "2) å®Œæˆæœ¬æ­¥éª¤åå¿…é¡»è¾“å‡º `Step Done: ...`ï¼ˆä¸€å¥è¯æ€»ç»“ï¼‰ã€‚\n"
                            "3) é™¤éè¿™æ˜¯æœ€åä¸€æ­¥ä¸”ä½ å·²ç¡®è®¤å®Œæˆæ‰€æœ‰æ­¥éª¤ï¼Œå¦åˆ™ä¸è¦è¾“å‡º Final Answerã€‚\n"
                        ),
                    )
                )

                for _ in range(step_max_iterations):
                    resp = await client.chat(all_messages)
                    content = resp.response

                    thought, action, action_input, final_answer = self._parse_response(content)
                    step_done = self._parse_step_done(content)

                    thoughts.append(
                        AgentThought(
                            thought=thought,
                            action=action if action else None,
                            action_input=action_input if action_input else None,
                        )
                    )

                    # æ­¥éª¤å®Œæˆï¼šè¿›å…¥ä¸‹ä¸€æ­¥
                    if step_done:
                        thoughts[-1].observation = step_done
                        all_messages.append(ChatMessage(role="assistant", content=content))
                        break

                    # æå‰ Finalï¼šè‹¥ä¸æ˜¯æœ€åä¸€æ­¥ï¼ŒæŒ‰ Step Done å¤„ç†ï¼ˆé¿å…å¡æ­»ï¼‰
                    if final_answer and idx < len(steps) - 1:
                        thoughts[-1].observation = "æ¨¡å‹æå‰è¾“å‡º Final Answerï¼Œå·²è§†ä¸º Step Done å¹¶ç»§ç»­æ‰§è¡Œåç»­æ­¥éª¤ã€‚"
                        all_messages.append(ChatMessage(role="assistant", content=content))
                        break

                    # æœ€åä¸€æ­¥å¯ç›´æ¥ Final
                    if final_answer and idx == len(steps) - 1:
                        return AgentResponse(
                            answer=final_answer,
                            thoughts=thoughts,
                            tool_calls=tool_calls,
                            model_name=self.config.model_name,
                            total_tokens=resp.total_tokens,
                        )

                    # å·¥å…·è°ƒç”¨
                    if action:
                        tool = self._clean_tool_name(action)
                        if allowed_step and tool not in allowed_step:
                            result = {"error": f"ToolNotAllowed: {tool} ä¸åœ¨æœ¬æ­¥éª¤å…è®¸å·¥å…·åˆ—è¡¨ä¸­"}
                        else:
                            result = await self._execute_tool(tool, action_input)

                        tool_calls.append(AgentToolCall(tool_name=tool, arguments=action_input, result=result))
                        result_str = json.dumps(result, ensure_ascii=False)
                        thoughts[-1].observation = result_str

                        all_messages.append(ChatMessage(role="assistant", content=content))
                        all_messages.append(ChatMessage(role="user", content=f"Observation: {result_str}"))
                        continue

                    # æ—  Action/Step Doneï¼šæç¤ºæ¨¡å‹æŒ‰æ ¼å¼ç»§ç»­
                    all_messages.append(ChatMessage(role="assistant", content=content))
                    all_messages.append(ChatMessage(role="user", content="è¯·ç»§ç»­æœ¬æ­¥éª¤ï¼šè¦ä¹ˆè°ƒç”¨å…è®¸çš„å·¥å…·ï¼ˆAction+Action Inputï¼‰ï¼Œè¦ä¹ˆè¾“å‡º Step Doneã€‚"))
                else:
                    # è¿­ä»£è€—å°½ï¼šå¼ºè¡Œè¿›å…¥ä¸‹ä¸€æ­¥
                    thoughts.append(AgentThought(thought=f"æ­¥éª¤{idx+1}è¶…å‡ºè¿­ä»£ä¸Šé™ï¼Œå¼ºåˆ¶è¿›å…¥ä¸‹ä¸€æ­¥", observation=""))

            # æ‰€æœ‰æ­¥éª¤æ‰§è¡Œå®Œï¼šè¦æ±‚è¾“å‡ºæœ€ç»ˆç­”æ¡ˆ
            all_messages.append(
                ChatMessage(
                    role="user",
                    content=(
                        "æ‰€æœ‰è®¡åˆ’æ­¥éª¤å·²æ‰§è¡Œå®Œæ¯•ã€‚ç°åœ¨è¯·è¾“å‡º Final Answerã€‚\n"
                        "- è‹¥å­˜åœ¨å·¥å…·è¿”å› error/å…³é”®æ•°æ®ç¼ºå¤±ï¼Œè¯·æ˜ç¡®è¯´æ˜å¹¶é™ä½ç½®ä¿¡åº¦ã€‚\n"
                        "- è‹¥å±äºä¹°å–å†³ç­–/æ“ä½œç­–ç•¥ç±»é—®é¢˜ï¼Œè¯·è¾“å‡ºâ€œå†³ç­–ä»ªè¡¨ç›˜ JSONâ€ä»£ç å—å¹¶åˆ—å‡º data_sourcesï¼ˆå®é™…è°ƒç”¨çš„å·¥å…·åï¼‰ã€‚\n"
                    ),
                )
            )

            for _ in range(final_max_iterations):
                resp = await client.chat(all_messages)
                content = resp.response
                thought, action, action_input, final_answer = self._parse_response(content)
                thoughts.append(AgentThought(thought=thought, action=action if action else None, action_input=action_input if action_input else None))

                if final_answer:
                    return AgentResponse(
                        answer=final_answer,
                        thoughts=thoughts,
                        tool_calls=tool_calls,
                        model_name=self.config.model_name,
                        total_tokens=resp.total_tokens,
                    )

                if action:
                    tool = self._clean_tool_name(action)
                    if union_allowed and tool not in union_allowed:
                        result = {"error": f"ToolNotAllowed: {tool} ä¸åœ¨è®¡åˆ’å…è®¸å·¥å…·é›†åˆä¸­"}
                    else:
                        result = await self._execute_tool(tool, action_input)
                    tool_calls.append(AgentToolCall(tool_name=tool, arguments=action_input, result=result))
                    result_str = json.dumps(result, ensure_ascii=False)
                    thoughts[-1].observation = result_str
                    all_messages.append(ChatMessage(role="assistant", content=content))
                    all_messages.append(ChatMessage(role="user", content=f"Observation: {result_str}"))
                    continue

                # æ²¡æœ‰ Final Answerï¼šç›´æ¥è¿”å›åŸå§‹å†…å®¹
                return AgentResponse(
                    answer=content,
                    thoughts=thoughts,
                    tool_calls=tool_calls,
                    model_name=self.config.model_name,
                    total_tokens=resp.total_tokens,
                )

            return AgentResponse(
                answer="æŠ±æ­‰ï¼Œæˆ‘éœ€è¦æ›´å¤šä¿¡æ¯æ‰èƒ½ç»™å‡ºæœ€ç»ˆç»“è®ºã€‚",
                thoughts=thoughts,
                tool_calls=tool_calls,
                model_name=self.config.model_name,
                total_tokens=0,
            )
        finally:
            await client.close()

    async def run_mode_stream(
        self,
        messages: List[ChatMessage],
        *,
        mode: str,
        max_plan_steps: int = 6,
        plan_candidates: int = 3,
        knowledge_context: str = "",
    ) -> AsyncGenerator[str, None]:
        """æŒ‰ mode æµå¼è¾“å‡ºäº‹ä»¶ï¼ˆPlan æ¨¡å¼ä¸ºäº‹ä»¶æµï¼Œä¸åš token çº§æµï¼‰ã€‚"""
        m = (mode or "").strip().lower()
        if m in {"", "agent"}:
            async for chunk in self.run_stream(messages):
                yield chunk
            return

        # do/thinkï¼šå…ˆäº§å‡º planï¼Œå†æŒ‰æ­¥éª¤æ‰§è¡Œï¼ˆäº‹ä»¶ç²’åº¦è¾“å‡ºï¼‰
        plan: Optional[dict] = None
        if m == "think":
            # å¤šå€™é€‰è§„åˆ’ â†’ è¯„ä¼°æŒ‘é€‰
            n = max(1, min(5, int(plan_candidates or 1)))
            hints = [
                "åå‘æ•°æ®å……åˆ†æ€§ï¼šå¤šç»´åº¦æ”¶é›†åå†ç»“è®º",
                "åå‘æˆæœ¬æ§åˆ¶ï¼šæœ€å°‘å·¥å…·è°ƒç”¨å®Œæˆä»»åŠ¡",
                "åå‘é£é™©æ’æŸ¥ï¼šä¼˜å…ˆå…¬å‘Š/èµ„é‡‘/å¼‚åŠ¨",
                "åå‘æŠ€æœ¯åˆ†æï¼šKçº¿/æŒ‡æ ‡/æ”¯æ’‘å‹åŠ›æ›´æ·±å…¥",
                "åå‘åŸºæœ¬é¢ï¼šè´¢åŠ¡/ä¼°å€¼/æœºæ„ç ”æŠ¥æ›´æ·±å…¥",
            ]
            plans: list[dict] = []
            for i in range(n):
                p = await self._create_plan(
                    messages,
                    mode="think",
                    max_steps=max_plan_steps,
                    candidate_hint=hints[i % len(hints)],
                    retrieval_context=knowledge_context,
                )
                if p and isinstance(p.get("steps"), list) and p.get("steps"):
                    plans.append(p)
            plan = await self._select_best_plan(messages, plans) or (plans[0] if plans else None)
        elif m == "do":
            plan = await self._create_plan(
                messages,
                mode="do",
                max_steps=max_plan_steps,
                retrieval_context=knowledge_context,
            )
        else:
            # æœªçŸ¥æ¨¡å¼ï¼šé™çº§ä¸º agent
            async for chunk in self.run_stream(messages):
                yield chunk
            return

        if not plan or not isinstance(plan.get("steps"), list):
            # è§„åˆ’å¤±è´¥ï¼šé™çº§ä¸ºåŸå§‹ ReAct
            async for chunk in self.run_stream(messages):
                yield chunk
            return

        yield json.dumps({"type": "plan", "plan": plan}, ensure_ascii=False)

        # å¤ç”¨ _run_with_plan çš„æ‰§è¡Œé€»è¾‘ï¼Œä½†ä»¥äº‹ä»¶æµå½¢å¼è¾“å‡ºå…³é”®èŠ‚ç‚¹
        from app.llm.client import LLMClient

        client = LLMClient(self.config)
        steps = plan.get("steps") or []
        all_messages = [ChatMessage(role="system", content=self._build_system_prompt())]
        kc = (knowledge_context or "").strip()
        if kc:
            all_messages.append(ChatMessage(role="system", content=f"ã€çŸ¥è¯†æ£€ç´¢ã€‘\n{kc}"))
        all_messages += list(messages or [])
        tool_names = self._tool_name_set()

        step_max_iterations = 4
        union_allowed: set[str] = set()
        for s in steps:
            if not isinstance(s, dict):
                continue
            allowed = s.get("allowed_tools", [])
            if not isinstance(allowed, list):
                continue
            for a in allowed:
                name = self._clean_tool_name(str(a or ""))
                if name in tool_names:
                    union_allowed.add(name)

        try:
            for idx, s in enumerate(steps):
                if not isinstance(s, dict):
                    continue
                title = str(s.get("title", "") or f"æ­¥éª¤{idx+1}").strip()
                goal = str(s.get("goal", "") or "").strip()
                allowed = s.get("allowed_tools", [])
                if not isinstance(allowed, list):
                    allowed = []
                allowed_step = []
                for a in allowed:
                    name = self._clean_tool_name(str(a or ""))
                    if name in tool_names:
                        allowed_step.append(name)
                allowed_step = list(dict.fromkeys(allowed_step))

                yield json.dumps(
                    {"type": "step_start", "step_index": idx + 1, "step_total": len(steps), "title": title, "goal": goal},
                    ensure_ascii=False,
                )

                all_messages.append(
                    ChatMessage(
                        role="user",
                        content=(
                            f"ç°åœ¨æ‰§è¡Œè®¡åˆ’æ­¥éª¤ {idx+1}/{len(steps)}ã€‚\n"
                            f"- æ ‡é¢˜: {title}\n"
                            f"- ç›®æ ‡: {goal}\n"
                            f"- å…è®¸å·¥å…·: {', '.join(allowed_step) if allowed_step else 'ï¼ˆä¸å…è®¸è°ƒç”¨å·¥å…·ï¼‰'}\n\n"
                            "è¦æ±‚ï¼šå®Œæˆæœ¬æ­¥éª¤åè¾“å‡º Step Done: ...ï¼›é™¤éæœ€åä¸€æ­¥å¦åˆ™ä¸è¦è¾“å‡º Final Answerã€‚"
                        ),
                    )
                )

                for _ in range(step_max_iterations):
                    resp = await client.chat(all_messages)
                    content = resp.response

                    thought, action, action_input, final_answer = self._parse_response(content)
                    step_done = self._parse_step_done(content)

                    if step_done:
                        yield json.dumps({"type": "step_done", "step_index": idx + 1, "content": step_done}, ensure_ascii=False)
                        all_messages.append(ChatMessage(role="assistant", content=content))
                        break

                    if final_answer and idx < len(steps) - 1:
                        yield json.dumps(
                            {"type": "step_done", "step_index": idx + 1, "content": "æ¨¡å‹æå‰è¾“å‡º Final Answerï¼Œå·²è§†ä¸ºæ­¥éª¤å®Œæˆ"},
                            ensure_ascii=False,
                        )
                        all_messages.append(ChatMessage(role="assistant", content=content))
                        break

                    if final_answer and idx == len(steps) - 1:
                        yield json.dumps({"type": "final_answer", "content": final_answer}, ensure_ascii=False)
                        return

                    if action:
                        tool = self._clean_tool_name(action)
                        yield json.dumps({"type": "tool_call", "tool": tool, "arguments": action_input}, ensure_ascii=False)
                        if allowed_step and tool not in allowed_step:
                            result = {"error": f"ToolNotAllowed: {tool} ä¸åœ¨æœ¬æ­¥éª¤å…è®¸å·¥å…·åˆ—è¡¨ä¸­"}
                        else:
                            result = await self._execute_tool(tool, action_input)
                        result_str = json.dumps(result, ensure_ascii=False)
                        yield json.dumps({"type": "observation", "content": result_str}, ensure_ascii=False)
                        all_messages.append(ChatMessage(role="assistant", content=content))
                        all_messages.append(ChatMessage(role="user", content=f"Observation: {result_str}"))
                        continue

                    all_messages.append(ChatMessage(role="assistant", content=content))
                    all_messages.append(ChatMessage(role="user", content="è¯·æŒ‰æ ¼å¼è¾“å‡º Step Done æˆ– Actionã€‚"))

            all_messages.append(ChatMessage(role="user", content="æ‰€æœ‰æ­¥éª¤å·²å®Œæˆã€‚è¯·è¾“å‡º Final Answerã€‚"))

            for _ in range(4):
                resp = await client.chat(all_messages)
                content = resp.response
                _, action, action_input, final_answer = self._parse_response(content)
                if final_answer:
                    yield json.dumps({"type": "final_answer", "content": final_answer}, ensure_ascii=False)
                    return
                if action:
                    tool = self._clean_tool_name(action)
                    yield json.dumps({"type": "tool_call", "tool": tool, "arguments": action_input}, ensure_ascii=False)
                    if union_allowed and tool not in union_allowed:
                        result = {"error": f"ToolNotAllowed: {tool} ä¸åœ¨è®¡åˆ’å…è®¸å·¥å…·é›†åˆä¸­"}
                    else:
                        result = await self._execute_tool(tool, action_input)
                    result_str = json.dumps(result, ensure_ascii=False)
                    yield json.dumps({"type": "observation", "content": result_str}, ensure_ascii=False)
                    all_messages.append(ChatMessage(role="assistant", content=content))
                    all_messages.append(ChatMessage(role="user", content=f"Observation: {result_str}"))
                    continue
                yield json.dumps({"type": "final_answer", "content": content}, ensure_ascii=False)
                return

            yield json.dumps({"type": "final_answer", "content": "æŠ±æ­‰ï¼Œæˆ‘éœ€è¦æ›´å¤šä¿¡æ¯æ‰èƒ½ç»™å‡ºæœ€ç»ˆç»“è®ºã€‚"}, ensure_ascii=False)
        finally:
            await client.close()

    def _build_system_prompt(self) -> str:
        """æ„å»ºç³»ç»ŸPrompt"""
        tools_desc = json.dumps(TOOLS, ensure_ascii=False, indent=2)
        return f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è‚¡ç¥¨åˆ†æåŠ©æ‰‹ï¼Œå¯ä»¥å¸®åŠ©ç”¨æˆ·æŸ¥è¯¢å’Œåˆ†æè‚¡ç¥¨æ•°æ®ã€‚

ä½ å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å·¥å…·ï¼š
{tools_desc}

å½“ç”¨æˆ·æé—®æ—¶ï¼Œè¯·æŒ‰ç…§ä»¥ä¸‹ReACTæ¨¡å¼å›ç­”ï¼ˆå¿…é¡»ä¸¥æ ¼éµå®ˆæ ¼å¼ï¼Œä¾¿äºç¨‹åºè§£æï¼‰ï¼š

1. Thought: æ€è€ƒç”¨æˆ·çš„é—®é¢˜ï¼Œåˆ†æéœ€è¦åšä»€ä¹ˆ
2. Action: é€‰æ‹©åˆé€‚çš„å·¥å…·
3. Action Input: å·¥å…·çš„è¾“å…¥å‚æ•°ï¼ˆJSONå¯¹è±¡ï¼›å°½é‡å†™æˆå•è¡Œï¼Œä¸è¦ç”¨ ``` ä»£ç å—ï¼‰
4. Observation: å·¥å…·è¿”å›çš„ç»“æœ
5. é‡å¤1-4ç›´åˆ°è·å–è¶³å¤Ÿä¿¡æ¯
6. Final Answer: ç»™å‡ºæœ€ç»ˆå›ç­”

æ³¨æ„ï¼š
- Thought åªå†™ç®€çŸ­è®¡åˆ’/ä¸‹ä¸€æ­¥ï¼ˆä¸è¦å±•å¼€æ¨ç†ç»†èŠ‚ï¼‰ã€‚
- Action å¿…é¡»æ˜¯å·¥å…·åˆ—è¡¨ä¸­çš„ name åŸæ ·ã€‚
- å¦‚æœå·¥å…·è¿”å› error å­—æ®µï¼Œè¯·åœ¨ Final Answer ä¸­è¯´æ˜å¤±è´¥åŸå› ï¼Œå¹¶ç»™å‡ºå¯è¡Œçš„æ›¿ä»£æ–¹æ¡ˆæˆ–ä¸‹ä¸€æ­¥å»ºè®®ã€‚
- ä¸è¦ç¼–é€ å·¥å…·è¿”å›çš„æ•°æ®ï¼›éœ€è¦æ•°æ®æ—¶å¿…é¡»å…ˆè°ƒç”¨å·¥å…·ã€‚
- å¦‚æœç”¨æˆ·çš„é—®é¢˜å±äºâ€œä¹°å–å†³ç­–/æ˜¯å¦å€¼å¾—/ç»™æ“ä½œç­–ç•¥/ä»“ä½/æ­¢æŸ/ç›®æ ‡ä»·â€ï¼Œè¯·åœ¨ Final Answer ä¸­å°½é‡è¾“å‡º**ç»“æ„åŒ–**ç»“è®ºï¼ˆå‚è€ƒ daily_stock_analysis çš„å†³ç­–ä»ªè¡¨ç›˜æ€è·¯ï¼‰ï¼š
  1) å…ˆç»™ä¸€å¥è¯ç»“è®ºï¼ˆğŸŸ¢/ğŸŸ¡/ğŸ”´/âš ï¸ + æ“ä½œå»ºè®®ï¼‰
  2) å†ç»™ä¸€ä¸ª JSON ä»£ç å—ï¼ˆå­—æ®µå°½é‡é½å…¨ä½†ä¿æŒç²¾ç‚¼ï¼‰ï¼Œç¤ºä¾‹å­—æ®µï¼š
     - sentiment_score: 0-100 æ•´æ•°
     - trend_prediction: å¼ºçƒˆçœ‹å¤š/çœ‹å¤š/éœ‡è¡/çœ‹ç©º/å¼ºçƒˆçœ‹ç©º
     - operation_advice: ä¹°å…¥/åŠ ä»“/æŒæœ‰/å‡ä»“/å–å‡º/è§‚æœ›
     - confidence_level: é«˜/ä¸­/ä½
     - key_levels: support/resistance/stop_loss/take_profitï¼ˆå°½é‡ç»™å‡ºæ•°å€¼æˆ–æ˜ç¡®æ¡ä»¶ï¼‰
     - checklist: 3-6 æ¡ âœ…/âš ï¸/âŒ æ£€æŸ¥é¡¹
     - risk_alerts / positive_catalysts: åˆ—è¡¨ï¼ˆå„ 1-5 æ¡ï¼‰
     - data_sources: æœ¬æ¬¡å®é™…è°ƒç”¨è¿‡çš„å·¥å…·ååˆ—è¡¨ï¼ˆä¸è¦æœæ’°ï¼‰
  3) è‹¥å…³é”®æ•°æ®ç¼ºå¤±ï¼ˆä¾‹å¦‚ç­¹ç /å…¬å‘Š/ç ”æŠ¥æ— æ•°æ®ï¼‰ï¼Œå¿…é¡»åœ¨ç»“è®ºä¸­æ˜¾å¼æ ‡æ³¨â€œä¸è¶³/ä¸å¯ç”¨â€ï¼Œå¹¶é™ä½ confidence_levelã€‚

è¯·ç”¨ä¸­æ–‡å›ç­”ã€‚
"""

    async def _execute_tool(self, tool_name: str, arguments: dict) -> Any:
        """æ‰§è¡Œå·¥å…· - å®Œæ•´å·¥å…·é›†"""
        from app.services.stock_service import StockService
        from app.services.market_service import MarketService
        from app.services.news_service import NewsService
        from app.services.search_service import SearchService
        from app.datasources.eastmoney import EastMoneyClient
        from app.utils.helpers import normalize_stock_code

        stock_service = StockService(self.db)
        market_service = MarketService(self.db)
        news_service = NewsService(self.db)
        search_service = SearchService(self.db)

        try:
            # 1. query_stock_price - å®æ—¶è‚¡ä»·
            if tool_name == "query_stock_price":
                codes = [normalize_stock_code(c) for c in str(arguments.get("stock_codes", "") or "").split(",") if c and c.strip()]
                quotes = await stock_service.get_realtime_quotes(codes)
                return {"stocks": [q.model_dump() for q in quotes]}

            # 2. query_stock_kline - Kçº¿æ•°æ®
            elif tool_name == "query_stock_kline":
                code = normalize_stock_code(str(arguments.get("stock_code", "") or ""))
                kline = await stock_service.get_kline(
                    code,
                    "day",
                    int(arguments.get("days", 30) or 30),
                )
                # è¿”å›æœ€è¿‘10æ¡æ•°æ®çš„æ‘˜è¦
                data = kline.data[-10:] if kline.data else []
                return {
                    "stock_code": kline.stock_code,
                    "stock_name": kline.stock_name,
                    "klines": [d.model_dump() for d in data]
                }

            # 3. query_stock_info - è‚¡ç¥¨ä¿¡æ¯æŸ¥è¯¢
            elif tool_name == "query_stock_info":
                results = await stock_service.search_stocks(
                    arguments["search_word"], limit=10
                )
                return {"results": [r.model_dump() for r in results]}

            # 4. get_financial_report - è´¢åŠ¡æŠ¥è¡¨
            elif tool_name == "get_financial_report":
                async with EastMoneyClient() as client:
                    return await client.get_financial_report(normalize_stock_code(str(arguments.get("stock_code", "") or "")))

            # 5. choice_stock_by_indicators - è‡ªç„¶è¯­è¨€é€‰è‚¡
            elif tool_name == "choice_stock_by_indicators":
                result = await search_service.search_by_words(arguments["words"])
                # é™åˆ¶è¿”å›æ•°é‡é¿å…tokenè¿‡å¤š
                stocks = result.get("results", [])[:20]
                return {
                    "conditions": result.get("conditions", []),
                    "stocks": stocks,
                    "total": result.get("total", 0)
                }

            # 6. query_market_news - å¸‚åœºèµ„è®¯
            elif tool_name == "query_market_news":
                news = await news_service.get_latest_news(
                    limit=arguments.get("limit", 20)
                )
                return {
                    "news": [
                        {"title": n.title, "content": n.content[:200] if n.content else "", "publish_time": str(n.publish_time)}
                        for n in news.items[:20]
                    ]
                }

            # 7. query_stock_news - è‚¡ç¥¨æ–°é—»æœç´¢
            elif tool_name == "query_stock_news":
                news = await news_service.search_news(
                    arguments["search_words"],
                    limit=20
                )
                return {
                    "news": [
                        {"title": n.title, "content": n.content[:200] if n.content else "", "publish_time": str(n.publish_time)}
                        for n in news.items[:20]
                    ]
                }

            # 8. query_interactive_qa - æŠ•èµ„è€…äº’åŠ¨é—®ç­”
            elif tool_name == "query_interactive_qa":
                async with EastMoneyClient() as client:
                    qa_list = await client.get_interactive_qa(
                        keyword=arguments.get("keyword", ""),
                        page=arguments.get("page", 1),
                        page_size=arguments.get("page_size", 20)
                    )
                return {"qa_list": qa_list}

            # 9. get_industry_research_report - è¡Œä¸šç ”ç©¶æŠ¥å‘Š
            elif tool_name == "get_industry_research_report":
                async with EastMoneyClient() as client:
                    reports = await client.get_industry_research_reports(
                        name=arguments.get("name", ""),
                        code=arguments.get("code", "")
                    )
                return {"reports": reports[:10]}

            # 10. query_economic_data - å®è§‚ç»æµæ•°æ®
            elif tool_name == "query_economic_data":
                data = await market_service.get_economic_data(
                    indicator=arguments.get("flag", "all"),
                    count=20
                )
                return data.model_dump() if data else {}

            # 11. query_bk_dict - æ¿å—/è¡Œä¸šå­—å…¸
            elif tool_name == "query_bk_dict":
                async with EastMoneyClient() as client:
                    industries = await client.get_industry_rank("change_percent", "desc", 50)
                    concepts = await client.get_concept_rank("change_percent", "desc", 50)
                return {
                    "industries": [{"code": i.bk_code, "name": i.bk_name} for i in industries.items],
                    "concepts": [{"code": c.bk_code, "name": c.bk_name} for c in concepts.items]
                }

            # 12. get_money_flow_rank - èµ„é‡‘æµå‘æ’å
            elif tool_name == "get_money_flow_rank":
                flow = await market_service.get_money_flow(
                    order=arguments.get("order", "desc"),
                    limit=arguments.get("limit", 20)
                )
                return {"stocks": [i.model_dump() for i in flow.items]}

            # 13. query_market_overview - å¸‚åœºæ¦‚è§ˆ
            elif tool_name == "query_market_overview":
                overview = await market_service.get_market_overview()
                return overview.model_dump() if overview else {}

            # 14. query_long_tiger - é¾™è™æ¦œ
            elif tool_name == "query_long_tiger":
                data = await market_service.get_long_tiger(arguments.get("trade_date"))
                if not data:
                    return {"items": [], "trade_date": arguments.get("trade_date", "")}
                payload = data.model_dump()
                # æ§åˆ¶è¿”å›æ¡æ•°ï¼Œé¿å… token çˆ†ç‚¸
                items = payload.get("items") or []
                payload["items"] = items[:20]
                return payload

            # 15. query_north_flow - åŒ—å‘èµ„é‡‘
            elif tool_name == "query_north_flow":
                days = int(arguments.get("days", 30) or 30)
                data = await market_service.get_north_flow(days)
                if not isinstance(data, dict):
                    return {"current": None, "history": []}
                history = data.get("history") or []
                if isinstance(history, list):
                    data["history"] = history[: min(len(history), 30)]
                return data

            # 16. query_industry_rank - è¡Œä¸šæ’å
            elif tool_name == "query_industry_rank":
                resp = await market_service.get_industry_rank(
                    sort_by=arguments.get("sort_by", "change_percent"),
                    order=arguments.get("order", "desc"),
                    limit=int(arguments.get("limit", 20) or 20),
                )
                payload = resp.model_dump() if resp else {"items": [], "update_time": ""}
                payload["items"] = (payload.get("items") or [])[:20]
                return payload

            # 17. query_concept_rank - æ¦‚å¿µæ¿å—æ’å
            elif tool_name == "query_concept_rank":
                resp = await market_service.get_concept_rank(
                    sort_by=arguments.get("sort_by", "change_percent"),
                    order=arguments.get("order", "desc"),
                    limit=int(arguments.get("limit", 20) or 20),
                )
                payload = resp.model_dump() if resp else {"items": [], "update_time": ""}
                payload["items"] = (payload.get("items") or [])[:20]
                return payload

            # 18. query_industry_money_flow - è¡Œä¸š/æ¦‚å¿µèµ„é‡‘æµå‘
            elif tool_name == "query_industry_money_flow":
                category = str(arguments.get("category", "hangye") or "hangye")
                sort_by = str(arguments.get("sort_by", "main_inflow") or "main_inflow")
                data = await market_service.get_industry_money_flow(category=category, sort_by=sort_by)
                if isinstance(data, dict) and isinstance(data.get("items"), list):
                    data["items"] = data["items"][:30]
                return data

            # 19. query_stock_money_rank - è‚¡ç¥¨èµ„é‡‘æµå…¥æ’å
            elif tool_name == "query_stock_money_rank":
                sort_by = str(arguments.get("sort_by", "main_inflow") or "main_inflow")
                limit = int(arguments.get("limit", 20) or 20)
                data = await market_service.get_stock_money_rank(sort_by=sort_by, limit=limit)
                if isinstance(data, dict) and isinstance(data.get("items"), list):
                    data["items"] = data["items"][:limit]
                return data

            # 20. query_volume_ratio_rank - é‡æ¯”æ’å
            elif tool_name == "query_volume_ratio_rank":
                min_ratio = float(arguments.get("min_ratio", 2.0) or 2.0)
                limit = int(arguments.get("limit", 20) or 20)
                data = await market_service.get_volume_ratio_rank(min_ratio=min_ratio, limit=limit)
                if isinstance(data, dict) and isinstance(data.get("items"), list):
                    data["items"] = data["items"][:limit]
                return data

            # 21. query_limit_stats - æ¶¨è·Œåœç»Ÿè®¡
            elif tool_name == "query_limit_stats":
                data = await market_service.get_limit_stats()
                if isinstance(data, dict):
                    # åå•åªä¿ç•™å‰ 50ï¼Œé¿å… token çˆ†ç‚¸
                    for k in ("limit_up_stocks", "limit_down_stocks"):
                        if isinstance(data.get(k), list):
                            data[k] = data[k][:50]
                return data

            # 22. get_stock_detail - è‚¡ç¥¨å…¨é‡è¯¦æƒ…
            elif tool_name == "get_stock_detail":
                code = normalize_stock_code(str(arguments.get("stock_code", "") or ""))
                detail = await stock_service.get_stock_detail(code)
                if not isinstance(detail, dict):
                    return {}
                # æ§åˆ¶è¿”å›ä½“ç§¯ï¼šåªè¿”å›å¸¸ç”¨å­—æ®µ
                return {
                    "quote": detail.get("quote"),
                    "basic": detail.get("basic"),
                    "fundamental": detail.get("fundamental"),
                    "rating": detail.get("rating"),
                    "shareholders": (detail.get("shareholders") or [])[:30] if isinstance(detail.get("shareholders"), list) else detail.get("shareholders"),
                    "dividend": (detail.get("dividend") or [])[:30] if isinstance(detail.get("dividend"), list) else detail.get("dividend"),
                    "concepts": detail.get("concepts"),
                }

            # 23. query_chip_distribution - ç­¹ç åˆ†å¸ƒ
            elif tool_name == "query_chip_distribution":
                code = normalize_stock_code(str(arguments.get("stock_code", "") or ""))
                resp = await stock_service.get_chip_distribution(code)
                return resp.model_dump()

            # 24. query_technical_analysis - æŠ€æœ¯åˆ†æ
            elif tool_name == "query_technical_analysis":
                code = normalize_stock_code(str(arguments.get("stock_code", "") or ""))
                days = int(arguments.get("days", 120) or 120)

                kline = await stock_service.get_kline(code, "day", days)
                klines = [d.model_dump() for d in (kline.data or [])]

                from app.services.technical_service import TechnicalService

                svc = TechnicalService()
                try:
                    result = await svc.analyze(code=code, klines=klines, stock_name=kline.stock_name)
                except Exception as e:
                    return {"error": f"æŠ€æœ¯åˆ†æå¤±è´¥: {e}", "stock_code": code, "kline_count": len(klines)}

                return {
                    "stock_code": result.code,
                    "stock_name": result.name,
                    "current_price": result.current_price,
                    "change_percent": result.change_percent,
                    "score": result.score,
                    "buy_signal": result.buy_signal.value,
                    "trend": {
                        "status": result.trend.status.value,
                        "ma_alignment": result.trend.ma_alignment,
                        "ma5": result.trend.ma_5,
                        "ma10": result.trend.ma_10,
                        "ma20": result.trend.ma_20,
                        "ma60": result.trend.ma_60,
                        "bias_ma5": result.trend.bias_5,
                        "bias_ma10": result.trend.bias_10,
                        "price_position": result.trend.price_position,
                    },
                    "macd": {
                        "dif": result.macd.dif,
                        "dea": result.macd.dea,
                        "macd": result.macd.macd,
                        "signal": result.macd.signal.value,
                    },
                    "rsi": {
                        "rsi_6": result.rsi.rsi_6,
                        "rsi_12": result.rsi.rsi_12,
                        "rsi_24": result.rsi.rsi_24,
                        "signal": result.rsi.signal.value,
                    },
                    "volume": {
                        "volume_ratio": result.volume.volume_ratio,
                        "volume_trend": result.volume.volume_trend,
                        "is_volume_breakout": result.volume.is_volume_breakout,
                        "avg_volume_5": result.volume.avg_volume_5,
                        "avg_volume_10": result.volume.avg_volume_10,
                    },
                    "support_resistance": {
                        "support_1": result.support_resistance.support_1,
                        "support_2": result.support_resistance.support_2,
                        "resistance_1": result.support_resistance.resistance_1,
                        "resistance_2": result.support_resistance.resistance_2,
                        "distance_to_support": result.support_resistance.distance_to_support,
                        "distance_to_resistance": result.support_resistance.distance_to_resistance,
                    },
                    "summary": result.summary,
                    "analysis_time": result.analysis_time.isoformat(),
                }

            # 25. query_stock_notices - å…¬å¸å…¬å‘Š
            elif tool_name == "query_stock_notices":
                code = normalize_stock_code(str(arguments.get("stock_code", "") or ""))
                limit = int(arguments.get("limit", 20) or 20)
                data = await news_service.get_stock_notices(code, limit=limit)
                if isinstance(data, dict) and isinstance(data.get("items"), list):
                    data["items"] = data["items"][:limit]
                return data

            # 26. query_stock_research_reports - ç ”æŠ¥
            elif tool_name == "query_stock_research_reports":
                code = normalize_stock_code(str(arguments.get("stock_code", "") or ""))
                limit = int(arguments.get("limit", 10) or 10)
                rows = await market_service.get_stock_research_reports(code, limit=limit)
                return {"stock_code": code, "items": (rows or [])[:limit], "total": len(rows or [])}

            # 27. query_hot_topics - çƒ­é—¨è¯é¢˜
            elif tool_name == "query_hot_topics":
                size = int(arguments.get("size", 20) or 20)
                return await news_service.get_hot_topics(size=size)

            # 28. query_hot_events - çƒ­é—¨äº‹ä»¶
            elif tool_name == "query_hot_events":
                size = int(arguments.get("size", 20) or 20)
                return await news_service.get_hot_events(size=size)

            # 29. search_web - è”ç½‘æœç´¢
            elif tool_name == "search_web":
                query = str(arguments.get("query", "") or "").strip()
                limit = int(arguments.get("limit", 8) or 8)
                if not query:
                    return {"items": [], "context": "query ä¸èƒ½ä¸ºç©º"}

                from app.services.news_search_service import NewsSearchService

                service = NewsSearchService(self.db)
                try:
                    items = await service.search(query=query, limit=limit)
                    context = service.format_as_context(query=query, items=items, max_items=min(5, limit))
                    return {
                        "query": query,
                        "context": context,
                        "items": [
                            {
                                "title": i.title,
                                "source": i.source,
                                "publish_time": i.publish_time.isoformat() if i.publish_time else "",
                                "url": i.url,
                                "content": (i.content or "")[:240],
                            }
                            for i in (items or [])[:limit]
                        ],
                    }
                finally:
                    await service.close()

            return {"error": f"Unknown tool: {tool_name}"}

        except Exception as e:
            return {"error": str(e)}

    @staticmethod
    def _clean_tool_name(name: str) -> str:
        """æ¸…ç†å·¥å…·åï¼Œé¿å…æ¨¡å‹è¾“å‡ºå¸¦å¼•å·/åå¼•å·å¯¼è‡´åŒ¹é…å¤±è´¥"""
        return re.sub(r"[`\"'\u201c\u201d]", "", (name or "").strip())

    @staticmethod
    def _extract_json_object(text: str) -> Optional[str]:
        """ä»æ–‡æœ¬ä¸­æå–ç¬¬ä¸€ä¸ª JSON å¯¹è±¡å­—ç¬¦ä¸²ï¼ˆ{...}ï¼‰ï¼Œç”¨äºè§£æ Action Input"""
        if not text:
            return None

        start = text.find("{")
        if start < 0:
            return None

        depth = 0
        in_string = False
        escape = False

        for i in range(start, len(text)):
            ch = text[i]
            if in_string:
                if escape:
                    escape = False
                elif ch == "\\":
                    escape = True
                elif ch == '"':
                    in_string = False
                continue

            if ch == '"':
                in_string = True
                continue

            if ch == "{":
                depth += 1
            elif ch == "}":
                depth -= 1
                if depth == 0:
                    return text[start : i + 1]

        return None

    def _parse_action_input(self, raw: str) -> dict:
        """è§£æ Action Inputï¼ˆå…¼å®¹å¤šè¡Œ/ä»£ç å—/å¤¹æ‚æ–‡æœ¬ï¼‰"""
        text = (raw or "").strip()
        if not text:
            return {}

        # 1) ä¼˜å…ˆæå–ä»£ç å—å†…å†…å®¹
        fence = re.search(r"```(?:json)?\s*(.*?)\s*```", text, flags=re.IGNORECASE | re.DOTALL)
        if fence:
            text = (fence.group(1) or "").strip()

        # 2) ç›´æ¥è§£æï¼ˆé€‚é…å•è¡Œ JSONï¼‰
        try:
            data = json.loads(text)
            return data if isinstance(data, dict) else {}
        except Exception:
            pass

        # 3) å°è¯•æå– JSON å¯¹è±¡ç‰‡æ®µ
        obj = self._extract_json_object(text)
        if not obj:
            return {}
        try:
            data = json.loads(obj)
            return data if isinstance(data, dict) else {}
        except Exception:
            return {}

    def _parse_response(self, content: str) -> tuple:
        """è§£æAgentå“åº”ï¼Œè¿”å›(thought, action, action_input, final_answer)"""
        thought = ""
        action = ""
        action_input_text = ""
        final_answer = ""

        lines = (content or "").strip().splitlines()
        current_section: Optional[str] = None

        def match_section(line: str) -> Optional[str]:
            # å…¼å®¹è‹±æ–‡/ä¸­æ–‡åˆ†éš”ç¬¦
            patterns = {
                "thought": r"^\s*(thought|æ€è€ƒ)\s*[:ï¼š]\s*",
                "action": r"^\s*(action|è¡ŒåŠ¨)\s*[:ï¼š]\s*",
                "action_input": r"^\s*(action\s*input|è¡ŒåŠ¨\s*è¾“å…¥)\s*[:ï¼š]\s*",
                "final_answer": r"^\s*(final\s*answer|æœ€ç»ˆå›ç­”|æœ€ç»ˆç­”æ¡ˆ)\s*[:ï¼š]\s*",
            }
            for key, pat in patterns.items():
                if re.match(pat, line, flags=re.IGNORECASE):
                    return key
            return None

        def strip_header(line: str) -> str:
            # å»æ‰ "xxx:" å‰ç¼€ï¼Œä¿ç•™åé¢çš„å†…å®¹
            return re.sub(r"^\s*[^:ï¼š]{1,30}\s*[:ï¼š]\s*", "", line).strip()

        for line in lines:
            section = match_section(line)
            if section:
                current_section = section
                inline = strip_header(line)
                if section == "thought":
                    thought = inline
                elif section == "action":
                    action = inline
                elif section == "action_input":
                    action_input_text = inline
                elif section == "final_answer":
                    final_answer = inline
                continue

            if current_section == "thought":
                thought = (thought + "\n" + line).strip() if thought else line.strip()
            elif current_section == "action_input":
                action_input_text = (action_input_text + "\n" + line).strip() if action_input_text else line.strip()
            elif current_section == "final_answer":
                final_answer = (final_answer + "\n" + line).strip() if final_answer else line.strip()

        action = self._clean_tool_name(action)
        action_input = self._parse_action_input(action_input_text)
        return thought.strip(), action.strip(), action_input, final_answer.strip()

    async def run(self, messages: List[ChatMessage]) -> AgentResponse:
        """è¿è¡ŒAgent"""
        from app.llm.client import LLMClient

        client = LLMClient(self.config)
        thoughts = []
        tool_calls = []

        try:
            # æ·»åŠ ç³»ç»ŸPrompt
            all_messages = [
                ChatMessage(role="system", content=self._build_system_prompt())
            ] + messages

            # æå‡è¿­ä»£ä¸Šé™ï¼šå¯¹é½â€œå…ˆå……åˆ†æ”¶é›†ä¿¡æ¯å†ç»“è®ºâ€çš„å·¥ä½œæ–¹å¼ï¼ˆä»éœ€æ§åˆ¶å·¥å…·è¾“å‡ºä½“ç§¯é¿å…æˆæœ¬å¤±æ§ï¼‰
            max_iterations = 7
            for _ in range(max_iterations):
                response = await client.chat(all_messages)
                content = response.response

                thought, action, action_input, final_answer = self._parse_response(content)

                thoughts.append(AgentThought(
                    thought=thought,
                    action=action if action else None,
                    action_input=action_input if action_input else None,
                ))

                if final_answer:
                    return AgentResponse(
                        answer=final_answer,
                        thoughts=thoughts,
                        tool_calls=tool_calls,
                        model_name=self.config.model_name,
                        total_tokens=response.total_tokens,
                    )

                if action:
                    # æ‰§è¡Œå·¥å…·
                    result = await self._execute_tool(action, action_input)
                    result_str = json.dumps(result, ensure_ascii=False)

                    tool_calls.append(AgentToolCall(
                        tool_name=action,
                        arguments=action_input,
                        result=result,
                    ))

                    # æ›´æ–°æœ€åä¸€ä¸ªthoughtçš„observation
                    thoughts[-1].observation = result_str

                    # æ·»åŠ observationåˆ°æ¶ˆæ¯
                    all_messages.append(ChatMessage(role="assistant", content=content))
                    all_messages.append(ChatMessage(role="user", content=f"Observation: {result_str}"))
                else:
                    # æ²¡æœ‰actionä¹Ÿæ²¡æœ‰final answerï¼Œå¯èƒ½æ˜¯æ ¼å¼é—®é¢˜
                    return AgentResponse(
                        answer=content,
                        thoughts=thoughts,
                        tool_calls=tool_calls,
                        model_name=self.config.model_name,
                        total_tokens=response.total_tokens,
                    )

            # è¶…è¿‡æœ€å¤§è¿­ä»£æ¬¡æ•°
            return AgentResponse(
                answer="æŠ±æ­‰ï¼Œæˆ‘éœ€è¦æ›´å¤šä¿¡æ¯æ‰èƒ½å›ç­”è¿™ä¸ªé—®é¢˜ã€‚",
                thoughts=thoughts,
                tool_calls=tool_calls,
                model_name=self.config.model_name,
                total_tokens=0,
            )
        finally:
            await client.close()

    async def run_stream(self, messages: List[ChatMessage]) -> AsyncGenerator[str, None]:
        """æµå¼è¿è¡ŒAgent"""
        from app.llm.client import LLMClient

        client = LLMClient(self.config)

        try:
            all_messages = [
                ChatMessage(role="system", content=self._build_system_prompt())
            ] + messages

            max_iterations = 7
            for iteration in range(max_iterations):
                full_content = ""

                async for chunk in client.chat_stream(all_messages):
                    full_content += chunk.content
                    yield json.dumps({
                        "type": "content",
                        "content": chunk.content,
                        "done": chunk.done,
                    })

                thought, action, action_input, final_answer = self._parse_response(full_content)

                if final_answer:
                    yield json.dumps({
                        "type": "final_answer",
                        "content": final_answer,
                    })
                    break

                if action:
                    yield json.dumps({
                        "type": "tool_call",
                        "tool": action,
                        "arguments": action_input,
                    })

                    result = await self._execute_tool(action, action_input)
                    result_str = json.dumps(result, ensure_ascii=False)

                    yield json.dumps({
                        "type": "observation",
                        "content": result_str,
                    })

                    all_messages.append(ChatMessage(role="assistant", content=full_content))
                    all_messages.append(ChatMessage(role="user", content=f"Observation: {result_str}"))
                else:
                    break
        finally:
            await client.close()
